{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lsOzTHnh5Hy",
        "outputId": "b8bee44b-8dfa-4657-825d-8359b9897a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.10/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.7.4)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.8.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install transformers\n",
        "!pip install pyngrok\n",
        "!pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_2tymhChsoK"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, render_template, session\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import faiss\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "from googletrans import Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYUds9YghsoM"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)\n",
        "app.secret_key = '123456'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOMdRec0hsoM"
      },
      "outputs": [],
      "source": [
        "# Función para generar embeddings\n",
        "def generar_embeddings(textos, tokenizer, modelo):\n",
        "    embeddings_list = []\n",
        "    for texto in textos:\n",
        "        inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = modelo.encoder(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings_list.append(embeddings)\n",
        "    return np.array(embeddings_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKKOBKebhsoM"
      },
      "outputs": [],
      "source": [
        "# Función para buscar la pregunta más similar\n",
        "def buscar_pregunta_similar(input_question, tokenizer, model, index, preguntas, respuestas, k=1):\n",
        "    input_embedding = generar_embeddings([input_question], tokenizer, model)\n",
        "    D, I = index.search(input_embedding.astype('float32'), k)\n",
        "    return [preguntas[i] for i in I[0]], [respuestas[i] for i in I[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmrOgxJhhsoM",
        "outputId": "cbb5cbe7-0569-46f2-8874-01e389273b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando el modelo...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(io.BytesIO(b))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado exitosamente.\n"
          ]
        }
      ],
      "source": [
        "# Función para generar respuesta\n",
        "def generar_respuesta(modelo, tokenizer, pregunta, respuesta):\n",
        "    input_text = f\"Pregunta: {pregunta}\\nRespuesta: {respuesta}\\nGenera una respuesta más detallada:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = modelo.generate(**inputs, max_length=150, num_return_sequences=1, temperature=0.7)\n",
        "    respuesta_generada=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    #return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    #traduccion\n",
        "    translator=Translator()\n",
        "    respuesta_traducida=translator.translate(respuesta_generada, dest=\"es\").text\n",
        "    return respuesta_traducida\n",
        "\n",
        "try:\n",
        "    # Cargar el modelo al iniciar la aplicación\n",
        "    print(\"Cargando el modelo...\")\n",
        "    loaded_data = joblib.load('/content/chatbot_model.joblib')\n",
        "    model = loaded_data['model']\n",
        "    tokenizer = loaded_data['tokenizer']\n",
        "    index = loaded_data['faiss_index']\n",
        "    preguntas = loaded_data['preguntas']\n",
        "    respuestas = loaded_data['respuestas']\n",
        "    print(\"Modelo cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkN47VMfBf71"
      },
      "outputs": [],
      "source": [
        "@app.route('/chat')\n",
        "def chat():\n",
        "    return render_template('chat.html', conversacion=session.get('conversacion', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzDi1NrpBG6h"
      },
      "outputs": [],
      "source": [
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def home():\n",
        "    if 'conversacion' not in session:\n",
        "        session['conversacion'] = []\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        pregunta = request.form['pregunta']\n",
        "        preguntas_similares, respuestas_similares = buscar_pregunta_similar(pregunta, tokenizer, model, index, preguntas, respuestas)\n",
        "        respuesta_detallada = generar_respuesta(model, tokenizer, preguntas_similares[0], respuestas_similares[0])\n",
        "\n",
        "        session['conversacion'].append({'pregunta': pregunta, 'respuesta': respuesta_detallada})\n",
        "        session.modified = True\n",
        "\n",
        "        return jsonify({'pregunta': pregunta, 'respuesta': respuesta_detallada})\n",
        "\n",
        "    return render_template('index2.html', conversacion=session.get('conversacion', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvGDuqaLkr8a",
        "outputId": "7aeb43c2-6c2e-4bec-a11c-ce3251073c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aplicación disponible en: NgrokTunnel: \"https://aaf4-34-72-15-56.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 21:36:56] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 21:36:56] \"GET /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 21:36:57] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 22:09:25] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 22:09:40] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 22:10:06] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Aug/2024 22:10:23] \"POST / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Reemplaza 'your_auth_token' con tu token de autenticación de ngrok\n",
        "    ngrok.set_auth_token('INGRESA_TU_TOKEN')\n",
        "\n",
        "     # Abre un túnel HTTP a través del puerto 5000\n",
        "    public_url = ngrok.connect(5000, \"http\")\n",
        "    print(f'Aplicación disponible en: {public_url}')\n",
        "\n",
        "    # Ejecuta el servidor Flask\n",
        "    app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
